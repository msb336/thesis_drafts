
\ExecuteOptions{letterpaper,oneside,12pt,onecolumn,final,openany}
\documentclass[12pt]{drexelthesis}
\usepackage[numbers]{natbib}
\usepackage{amssymb}
\usepackage{siunitx}
\usepackage{placeins}
\let\Oldsection\section
\renewcommand{\section}{\FloatBarrier\Oldsection}

\let\Oldsubsection\subsection
\renewcommand{\subsection}{\FloatBarrier\Oldsubsection}

\let\Oldsubsubsection\subsubsection
\renewcommand{\subsubsection}{\FloatBarrier\Oldsubsubsection}




\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{listings}
%\usepackage[section]{placeins}
\graphicspath{{images/}}

\title{Automated Conversion of 3D Point Clouds to FEA Compatible Meshes}
\author{Matthew S.~Brown}
\advisor{Antonios~Kontsos, Ph.~D.~}


\begin{document}
\maketitle
\begin{preliminary}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\setcounter{page}{2}

\date{} % Purposely left blank

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{dedication}

\end{dedication}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{acknowledgements}

\end{acknowledgements}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\mytableofcontents\newpage


\setlength{\baselineskip}{0.5\baselineskip}
\listoftables \newpage


\listoffigures \newpage
\setlength{\baselineskip}{2.0\baselineskip}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\addcontentsline{toc}{chapter}{Abstract}
\begin{abstract}
This paper outlines a method to utilize machine learning in conjunction with advanced meshing techniques to autonomously segment raw point cloud data and reconstruct the resulting segments into simply connected volumetric meshes.

\end{abstract}

\clearpage
\end{preliminary}
\thispagestyle{empty}
\newpage


\setcounter{page}{0}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% CHAPTER:                                                         INTRODUCTION
\chapter{Introduction}
\label{chap:introduction}

Surface reconstruction from a 3D point cloud is not a novel problem. In the past, groups have developed meshing algorithms for digital art replication, geographical topology analysis, and – more recently – structure health monitoring. All these processes, however, do not provide a general method to automate the entire pipeline between point-cloud collection and simple CAD geometry. We present a solution to this problem in the form of a robust and autonomous process for filtering, segmentation, and meshing of raw 3D point clouds.

%Related Works
\section{Related Work}
\label{sec:related}
[Introductory filler]
%%
\subsection{Geographical Topography Mapping}
\label{subsec:topographymapping}
In 2009, José Lerma and his team of archaeologists began using Terrestrial laser scanning in tandem with close proximity photogrammetry to render high resolution 3D surface models of ancient caves in Spain. Lerma et. al. are general in their description of their meshing method, which is most likely due to their non-computer programming oriented backgrounds. However, their pipeline involves sensor fusion between their laser scanner, which returns a pure point cloud with an origin at the center of the instrument, and deduced point clouds from photogrammetry data. The result is an impressive, high resolution surface mesh that accurately captures the features relevant to an archaeologist, but provide no useful information in terms of structural health monitoring \cite{RN50}.

In 2014, Sebastian Siebert implemented similar technology mounted to UAVs to provide 3D mapping of earthwork projects for surveyors \cite{RN48}.
%%
\subsection{Building Informational Modeling}
\label{subsec:BIM}
%[3]
\cite{RN30}
%%
\subsection{Aerial Scanning for GPS Overlay}
\label{subsec:UAVscanning}
%%
\subsection{Semi-Automated Point Cloud to FEA modeling}
\label{subsec:SemiFEA}
\cite{RN29} \cite{RN31} \cite{RN38} \cite{RN54}
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Chapter: 		Background
\chapter{Background}
\label{chap:background}
%%
\section{Collection of Point Cloud Data}
\label{sec:PCcollection}
There are many ways of collecting point cloud data, ranging from implicit methods where the collection tool does not return direct xyz point data, such as stereogrammetry and structure from motion, to explicit methods where the direct return is a 3-dimensional position output, such as LiDAR, laser scanning and ultrasonic sensing. Each collection technique has its own set of parameters, accuracy ratings, and speed of collection / calculation.
\subsection{Implicit collection methods: Stereogrammetry and Structure from Motion}
\label{subsec:implicit}
\cite{RN20}
\subsection{Explicit Methods: LiDAR, Laser Scanning, and Ultrasonic Sensing}
\label{subsec:explicit}
%%
\section{Sensor Fusion}
\label{sec:fusion}
%%
\section{Point Cloud Pre-processing Methods}
\label{sec:preprocessing}
\subsection{Registration}
\subsubsection{Position Data}
\subsubsection{Intrinsic Shape Signatures}
To stitch individual frames together, distinctive, repeatable features from each frame are found, and the most likely transformation between the frames is calculated via RANSAC estimation. There are numerous ways to classify distinctive features, but in this paper, we will focus on Intrinsic Shape Signatures due to its reliability and computational efficiency.
An intrinsic shape signature consists of two things:
\begin{itemize}
	\item An intrinsic reference frame
	\item A highly discriminative feature vector encoding the 3D shape characteristics
\end{itemize}
\subparagraph{Intrinsic Reference Frame Calculation}
\begin{enumerate}
	\item Compute a weight for each point $p_{i}$ inversely related to the number of points within 2-norm distance $r_{density}$:
	
		\begin{equation}
		w_{i}=  \frac{1}{||p_{j} \mid |p_{j}-p_{i}| < r_{density}||}
		\end{equation}
		This weight is used to compensate for uneven sampling of the 3D points, so that points at sparsely sampled regions contribute more than points at densely sampled regions. 

	\item Compute a weighted scatter matrix $cov(p_{i})$ for $p_{i}$ using all points $p_{j}$ within distance $r_{frame}$:
	
		\begin{equation}
		cov(p_{i})= \sum{|p_{j}-p{i}| < r_{frame}}\frac{w_{j}(p_{j}-p_{i})(p_{j}-p{i})^{T}}{ \sum{|p_{j}-p{i}| < r_{frame}}w_{j}}
		\end{equation}
		
	\item Compute the covariance matrix eigenvalues in order of decreasing magnitude and their resulting eigenvectors.
	\item $p_{i}$ is now the origin of the intrinsic frame, with $e^{1}$, $e^{2}$, and their cross product as the $x$, $y$, and $z$ axes, respectively \cite{RN60}.
\end{enumerate}

\subparagraph{3D Shape Feature Extraction}
The goal of the extraction is to create a view invariant “feature” vector providing us with some unique qualities about the point relationships within the intrinsic reference frame. At each point in the point cloud, or in increments of voxel stride size s, we build a sphere of some desired radius r centered at pi and divide it into 66 distinct partitions in angular space ($\theta, \psi$). A distinctive feature vector with 66 values is then computed by summing the radial distances $\rho_{i}$ in each bin \cite{RN60}.

\begin{figure}[!ht]
\centering
\includegraphics{ISSfeaturevector2.jpg}  \includegraphics{ISSfeaturevector.jpg}
\caption[Intrinsic Shape Signature feature vectors]{\centering Feature vector calculation via spherical bin decomposition \cite{RN60}.}
\end{figure}

\subsection{Flitering}
To minimize the amount of noise in the resulting dataset, a statistical approach requiring each point to have k neighbors within d standard deviations from the mean density radius of the cloud. This allows for controlled outlier removal, and a smoother cloud with fewer sharp edges.
\begin{equation}
	P_{x} = p_{i} \mid \sum_{j=1}^n |p_{j} - p_{i}| \leq (r_{density} + d) \geq k
\end{equation}

\begin{figure}[!ht]
\centering
\includegraphics[width=2in]{l_block_pt_cloud10pnoise.jpg} \includegraphics[width=2in]{l_block_pt_cloud10pnoiseFILTEREDk100std05.jpg}
\caption[Effects of noise filter on simulated point cloud objects]{\centering The effects of noise filtering on a simulated L block with 10\% induced noise.}
\end{figure}


\subsection{Down-sampling}
Down-sampling is the process of fixing a point cloud’s mean density to a voxel of size n. This is done by iterating the voxel throughout the cloud’s entire volume and replacing all points occupying a voxel with a single point in the mean position of the voxel.

\begin{figure}[!ht]
\centering
\includegraphics[width=2in]{l_block_pt_cloud} \includegraphics[width=2in]{l_block_pt_cloudDOWNSAMPLE025.jpg}
\caption[Effects of down-sampling on simulated point cloud objects]{\centering A simulated L block point cloud down-sampled with voxel size = 0.25 $cm^{3}$.}
\end{figure}


\subsection{Handling Occlusion} 
Occlusion is a common issue in the perception world, defined by the lack of information in an image / 3D scan due to other objects blocking a direct view. A simple example: In 3D scene reconstruction from images, it is impossible to accurately reconstruct the contents inside an opaque box because we cannot see inside the box. This is occlusion. In the field, it is nearly impossible to fully avoid occluded datasets when scanning an object via LiDAR equipped UAVs. It is crucial to be able to develop methods to alleviate this problem.

\begin{figure}[!ht]
\centering
\includegraphics{occluded_man.jpg}
\caption[Demonstration of occlusion]{\centering Section of a building occluded by an object in the foreground \cite{RN13}.}
\end{figure}

\subsubsection{Range Segementation}
\label{subsubsec:rangeseg}
In urban scanning situations, it is nearly impossible to avoid scan noise. This can come in the form of humans passing by, parked cars, street lights, other buildings, trees, bushes, and a slew of other objects that may come between the scanner and the desired object. Biasuttia et. al. propose a way to cast these interruptions to the surface they desire to map by converting the xyz point cloud to a range image --- a three parameter map of distance $r$ from the device plotted against $\theta$ and $\phi$, the rotation about the $z$ and $x$ axes, respectively.

Once the points are cast to a range image, a range histogram is created. The histogram is segmented into $S$ classes, and the centroid of each class is calculated using the following equation.

\begin{equation}
	C_{s}^{i} = \frac{\sum_{b\in C_{s}^{i}} b  \times h_{s}(b)}{\sum_{b\in C_{s}^{i}}h_{s}(b)}
\end{equation}

Any centroids within some user-defined distance, $\tau$, are merged as a single cloud.

\begin{equation}
	d(C_{s}^{i}, C_{r}^{j}) = |C_{s}^{i} - C_{r}^{j}|
\end{equation}

An algorithm built under the pretext of Gaussian diffusion is then used to project points with a significantly different normals to conform with their range image neighbors. This approach requires structured data that is also time-stamped. In the equation below, $u$ represents the $(\phi, \theta)$ coordinates of a point the merged dataset, and $\Omega$  represents the full range image of the merged dataset, and $\eta$ represents the orthogonal projection of each pixel in the range image. The aim is to solve the following disocclusion problem.

\begin{equation}
		\begin{cases}
			\frac{\partial u}{\partial t} - \Delta u = 0 \in \Omega \times (0,T) \\
			u(0,x) = u_{0}(0) \in \Omega
		\end{cases}
\end{equation}

When scanning large objects, this method proves to be quite effective at removing sources of noise that are significantly smaller than the object being scanned. For buildings, bridges, and large-scale structures, this is a necessary first step in removing excess noise / unnecessary information from the cloud \cite{RN13}.


\subsubsection{Informed Shape Estimation}
Occlusion is the cause of two main problems. The first being the unnecessary information provided by objects blocking the instruments view to our desired target, which is dealt with via the diffusion of objects from the foreground into the background via the range segmentation method show in the previous section. The second --- far more relevant to the approach outlined in this thesis --- is the lack of complete information provided by the sensor. This can be most easily explained by making an analogy to photography. If one takes a picture of the front of a box, there is no possible way to say with certainty what the back of the box looks like. In 3D point processing, the problem is the same. Informed shape estimation attempts to tackle this problem by applying a shape with known parameters to the object point cloud and modifying the shape parameters to minimize the following cost function:

\begin{equation}
	C = \sum_{i=0}^{N}{p(i) - p_{proj}(i)}
\end{equation}

Where $N$ represents the number of points in the set, $p(i)$ represents the $i^{th}$ point in the set, and $p_{proj}(i)$ represents the closest point on the applied shape who's unit normal vector matches within some tolerance, $\tau$.

\begin{equation}
	p_{proj}(i) = arg_{min} \frac{p(i) \bullet l(j)}{||l(j)||} \in p_{normal}(i) \bullet l_{normal}(j) \geq \tau
\end{equation}

\begin{equation}
	\label{eq:newtonraph}
	x_{k+1} = x_{k} - \frac{C(x_{k})}{C'(x_{k})}
\end{equation}

Using a Newton Raphson interative regression, shown in equation \ref{eq:newtonraph} the cross section parameters are modified to minimize the cost required to projection points to the estimated surface. This technique is iterated throughout the long axis of the object, allowing for crucial information such as deformation to be retained. The power in this method is in it's ability to fill information in heavily occluded areas, at the cost of having a narrow scope. Informed shape estimation makes the following assumptions:

\begin{enumerate}
	\item Objection deformation is planar. Calculating the length plane removes the information involving multi-axial deformation.
	\item Cross-section is undamaged thoughout the length of the object. Information regarding damage which alters the objects cross section will be lost when points are projected to the estimated cross section.
	\item The target object can be defined via simple parameters such as length, height, width, and thickness.
\end{enumerate}

\begin{figure}[!ht]
	\centering
		\includegraphics[width=3in]{cross-section-estimation/flowchart.jpg}
	\caption[Flow chart of informed shape estimation algorithm]{\centering A flowchart of the informed shape estimation algorithm.}
\end{figure}

In many object scanning situations, it is not unreasonable to assume the cross-section of the object is known. Forcing the point cloud to conform to a uniform shape allows for avoidance of heavy amounts of noise.




 


%%
\section{Machine Learning for Object Recognition and Segmentation}
\label{sec:machinelearning}
\subsection{Supervised Methods --- Neural Networks}
\subparagraph{Definitions}
It is difficult to define how a Convolutional Neural Network works without first defining a convolution. To detect distinctive features in a dataset – from a machine’s perspective – the dataset needs to be modified to enunciate those features. Key features in machine vision include edges, corners, and areas with distinctive geometry. Convolutions are the key to bringing these features to the forefront of the image.
A convolution kernel is a weighted square matrix of dimensions m, and depth equaling the rank of the feature space of the dataset. The kernel acts as a filter for the image as it strides from supervoxel to supervoxel. At each step, the dot product of the kernel with data values inside the current super-voxel provide a convolved image of the dataset while retaining characteristic features. The equation below illustrates the math behind a convolution kernel. $C$ represents the convolved image, and $x_{i}$ and $w(i)$ are the $i^{th}$ point and weight of $N$ points located inside the voxel.

\begin{equation}
	C_{new} = \sum_{i}^{N}  w(i) x_{i}
\end{equation}

Another type of convolution is called pooling, or subsampling. This convolution steps through the dataset with a stride value greater than one, resulting in a smaller image of the original set. In pooling, the kernel will pull either the largest intensity from each super-voxel, or the average intensity of the data points inside the super-voxel. This allows for the machine to decrease the size of the dataset while maintaining distinctive features.

\begin{equation}
	C_{new} = \frac{\sum_{i}^{N}  x_{i}}{N}
\end{equation}

\subparagraph{Convolutional Neural Networks}
Convolutional Neural Networks (CNNs) are modeled after the visual cortex in the brain. They consist of layers of convolutional networks. Each network contains “neurons” with simple feature reception fields. Through layers upon layers of these networks, objects can be classified. The biases and weights on these networks can be adjusted based on learning algorithms REF 1 in proposal. CNNs have become the standard in feature classification for image processing, but the accuracy that they provide comes at the price of processing time. 
Every CNN can be broken down into the following steps: Convolution, max/mean pooling, activation function, fully connected layer, repeat. The diagram below illustrates the overarching structure of a basic Convolutional Neural Network:
\begin{figure}[!ht]
	\centering
		\includegraphics[width=6in]{cnn.png}
	\caption[High level flow chart of a convolutional neural network]{\centering Map of a convolutional neural network with two hidden layers [ref]}
\end{figure}

In the case of point cloud processing, the input is a raw xyz set of some size n x 3. The first step in the network is a series of convolutions of the dataset. Each kernel in the layer contains m x m x 3 trainable weights, which are iteratively modified using a steepest descent numerical solver during the training phase of the system. The convolved images are stacked in a block, called a feature map, or convolutional layer. From there, the convolved images are pooled (or subsampled) to condense the size of the image stack. 
At this point, there is a large stack of feature maps draw from the original input dataset. In a simple linear system, these features are combined into a single weighted summation function, where the input is each individual feature value, and the output is a vector representing the probability of the image belonging to a certain class.

\begin{equation}
\begin{bmatrix}
C_{1} \\
\vdots \\
C_{n}
\end{bmatrix}  =  \begin{bmatrix} w_{1,1} & \hdots & w_{1,m+1} \\ \vdots & \ddots & \hdots \\ w_{n,1} & \hdots & w_{n, m+1} \end{bmatrix}  \begin{bmatrix} f_{1} \\ \vdots \\ f_{n} \\ 1 \end{bmatrix}
\end{equation}

The equation above represents the transformation from feature space to the “fully connected layer.” $C_{i}$ represents the probability of the input image belonging to class $i$, and $f_{i}$ represents the value of the ith feature in the feature map. 
Linear classification methods limit the versatility of the CNN, as many object distinctions do not follow a linear pattern in $n$-dimensional space. To account for this, most CNNs – including the ones utilized in this paper – incorporate a de-linearizing element dubbed the “activation function.” The activation function applies a nonlinear operation to the values in the convolution layer, which allows for the CNN to become a very powerful nonlinear fit function. Typical activation functions include the hyperbolic tangent function, the sigmoid function, and – most popularly – the rectifier function. Each of these functions are show in the figure below:

\begin{figure}[!ht]
\centering
\includegraphics[width=5in]{cnnReLu.png}
\caption[Common CNN non-linear activation functions]{\centering Visualization of commonly used non-linear activation functions}
\end{figure}

From input to output, all CNNs have the same skeleton structure, with a varying number of layers between the raw input and the fully connected layer:

\begin{figure}[!ht]
\centering
\includegraphics[width=6in]{cnn_flow.png} 
\caption[Block diagram of CNN flow]{\centering Block diagram of a CNN$'$s skeleton structure}
\end{figure}

For the system to be accurate, the weights for each convolution and connection must be trained. This training is done through a process called backpropagation. An image-set of known classifications is fed to the untrained system, and the error between the system’s classification and the true classification of each image is used to update the weights iteratively until the machine’s class prediction closely resembles ground truth. Most algorithms use numerical solving methods to sharply diminish the number of iterations required for convergence. The “gradient descent” method is commonly used in the machine learning world due to its rapid convergence properties and low computational complexity. The method is shown below:

\begin{equation}
	x_{k+1} = x_{k} - \gamma \nabla F(x_{k})
\end{equation}

\begin{equation}
 F(x) =
\begin{bmatrix}
C_{1} \\
\vdots \\
C_{n}
\end{bmatrix}  -  \begin{bmatrix} w_{1,1} & \hdots & w_{1,m+1} \\ \vdots & \ddots & \hdots \\ w_{n,1} & \hdots & w_{n, m+1} \end{bmatrix}  \begin{bmatrix} f_{1} \\ \vdots \\ f_{n} \\ 1 \end{bmatrix} 
\end{equation}

The speed, accuracy, and versatility of a CNN are functions of the number of hidden layers, the size of the convolutional layers, the type of activation functions, and the size and versatility of the training dataset \cite{RN7}.

\subsection{Unsupervised Methods}
With our goal being to isolate specific objects in a structural health monitoring setting, the ideal segmentation method is a supervised learning algorithm, such as a convolutional neural network, that semantically parses the point cloud based on a training set of pre-defined cloud objects \cite{RN72}. However, due to time limitations, and a lack of training data relevant to our objects of interest, this is not possible. Instead we explore a series of unsupervised clustering methods on the assumption that objects are distinct enough in relative cloud neighborhoods to be properly segmented.

\subsubsection{K-means Clustering}
K-means clustering is an iterative method that groups $n$-dimensional datasets into $k$ clusters based on a minimization of the Euclidean distance cost function $|x-c|^{2}$. Initially, $k$ centroids are placed randomly inside the dataset, and all data points are placed in bins $S$ depending on which centroid minimizes their cost function. At each iteration, the cluster centroids $c_{i}$ are re-calculated. Criteria for convergence is a maximum Euclidean distance change $\sigma$ between centroid position $c_{n}$ and $c_{n+1}$.
\begin{equation}
	arg min_{S} \sum{i=1}^{k} \sum{x \in S_{i}} |x - c_{i}|^{2}
\end{equation}

[PROS, CONS, AND GENERAL USAGE CASES]

\subsubsection{Fuzzy C-means Clustering}

Fuzzy C-means (FCM) is very similar to K-means clustering. Once again, points are iteratively grouped to k centroids based on their Euclidean distance to the centroid. The significant difference is that points do not belong exclusively to a single group. Instead, points are weighted by their degree of belonging in each cluster.

\begin{equation}
	c_{k} = \frac{\sum{x}w_{k}(x)^{m}x}{\sum{x}w_{k}(x)^{m}}
\end{equation}

Each point is provided a weight vector w [0, 1] for its likelihood of belonging in each cluster, where the weight function is as follows:

\begin{equation}
	w_{ij} = \frac{1}{\sum_{k=1}^{c}(\frac{|x_{i}-c_{j}|}{|x_{i}-c_{k}|})^{\frac{2}{m-1}}}
\end{equation}

[PROS, CONS, AND GENERAL USAGE CASES]

\subsubsection{Aggomerative and Dvisive Hierarchical Clustering}
In Agglomerative clustering, each point is initially considered its own cluster. Iteratively, the points are grouped together based on a user-defined cost function – in our case, Euclidean distance. The exit conditions for this method are either convergence upon a set of clusters, or a predefined number of clusters.
Divisive clustering approaches the clustering problem in an exactly opposite fashion. The algorithm initializes the dataset as a single cluster and iteratively splits the remaining clusters until reaching the same exit conditions as the Agglomerative method.


\begin{figure}[!ht]
\centering
\includegraphics[width=4in]{divisiveAgglomerative.png}
\caption[Comparison of divisive and agglomerative clustering methods]{Comparison of divisive and agglomerative hierarchical clustering}
\end{figure}


\subsubsection{Euclidean Distance Clustering}
Perhaps the simplest of the algorithms listed above, Euclidean distance clustering operates on the pretense that objects are separated significantly enough spatially from another that clustering points based on their proximity to other points in the cloud is sufficient to properly segment the dataset. This algorithm involves no iterative process, and requires two inputs: Maximum point-to-point distance $r$, and minimum number of points per cluster $k$.

\subsubsection{Comparison of Methods}

 \begin{table}[h!]
     \begin{center}
           \caption[Comparison of Unsupervised clustering methods]{Comparision of unsupervised clustering methods on various simulated point clouds}
     \begin{tabular}{ | c | p{3cm} | p{3cm} | p{3cm} | p{3cm} | }
     \hline
      K-means & Fuzzy C-means & Agglomerative & Divisive & Euclidean \\ 
      \hline
      		\includegraphics[width=2cm]{2d-cluster-tests/k-means/concentric.jpg}
      & 
      		\includegraphics[trim={0 0 0 2cm},clip, width=2cm]{2d-cluster-tests/fcm/concentric.jpg}
      & 
      		\includegraphics[trim={0 1cm 0 1cm},clip, width=2cm]{2d-cluster-tests/agglomerative/concentric.jpg}
      &

      &
      		\includegraphics[width=2cm]{2d-cluster-tests/euclidean-distance/concentric.jpg}
      \\ \hline
      
            \includegraphics[width=1.5cm]{2d-cluster-tests/k-means/lines.jpg}
      & 
             \includegraphics[width=1.5cm]{2d-cluster-tests/fcm/lines.jpg}    
      & 
             \includegraphics[trim={0 1cm 0 0cm},clip,width=1.5cm]{2d-cluster-tests/agglomerative/lines.jpg}    
      &

      &
             \includegraphics[trim={0 1cm 0 1cm},clip, width=1.5cm]{2d-cluster-tests/euclidean-distance/lines.jpg}    
      \\ \hline
      
           \includegraphics[trim={0 1cm 0 1cm},clip,width=1.5cm]{2d-cluster-tests/k-means/blob.jpg}
      & 
           \includegraphics[trim={0 0cm 0 0cm},clip,width=1.5cm]{2d-cluster-tests/fcm/blob.jpg} 
      & 
           \includegraphics[trim={0 1cm 0 1cm},clip,width=1.5cm]{2d-cluster-tests/agglomerative/blob.jpg} 
      &

      &
           \includegraphics[trim={0 1cm 0 1cm},clip,width=1.5cm]{2d-cluster-tests/euclidean-distance/blob.jpg} 
      \\ \hline
      
            \includegraphics[trim={0 1cm 0 1cm},clip,width=1.5cm]{2d-cluster-tests/k-means/solid_circles.jpg}
      & 
            \includegraphics[trim={0 1cm 0 1cm},clip,width=1.5cm]{2d-cluster-tests/fcm/circles.jpg}
      & 
            \includegraphics[trim={0 1cm 0 1cm},clip,width=1.5cm]{2d-cluster-tests/agglomerative/solid_circles.jpg}
      &

      &
            \includegraphics[trim={0 1.5cm 0 1.5cm},clip,width=1.5cm]{2d-cluster-tests/euclidean-distance/solid_circles.jpg}
      \\ \hline
      
            \includegraphics[trim={0 1cm 0 1cm},clip,width=1.5cm]{2d-cluster-tests/k-means/plane.jpg}
      & 
            \includegraphics[trim={0 0cm 0 1cm},clip,width=1.5cm]{2d-cluster-tests/fcm/plane.jpg}
      & 
            \includegraphics[trim={0 1cm 0 1cm},clip,width=1.5cm]{2d-cluster-tests/agglomerative/plane.jpg}
      &
      
      &
            \includegraphics[trim={0 1cm 0 1cm},clip,width=1.5cm]{2d-cluster-tests/euclidean-distance/plane.jpg}
      \\ \hline
      
      \end{tabular}
      \end{center}
      \end{table}


\subsection{Converting a Discrete Point Cloud to a Bounded Area Surface Mesh} 
\subsubsection{Definition of a Surface Mesh}
Surface meshes, in simple terms, are a series of connected areas defined by their vertices in $n$-dimensional space. The areas are typically triangles or tetrahedrons, and form together to define a continuous surface area using discrete data points. There are many ways to store surface mesh objects, but they all require two components:

\begin{enumerate}
	\item A set of vertices, $v$, occupying $n$-dimensional space $\mathbb{R}^{n}$.
	\item A set of connection indices $I$, which dictate how the vertices connect to one another.
\end{enumerate}

In a fully bounded surface mesh, the combination of each triangulation object creates a shell, which represents the surface area of the shape.


\subsubsection{Definition of a Volume Mesh}
A volume mesh is defined with the same parameters as a surface mesh, with the key difference being that it defines the entire exterior and interior of the object.
One application --- and the target application for this thesis --- is finite element analysis, a numerical method for solving structural analysis problems through discretization of individual elements in an $n$-dimensional geometry and calculating their relative parameters in reference to their connected neighbors.

\subsubsection{Definition of a Boundary Representation (B-REP)}
Boundary Representations, often called the ''skin'' of a solid object, consist of two parts: topology and geometry. The topological portion is made from the faces and vertices created by a surface mesh. In the boundary representation of an object, the edges bounding a triangulation object -- or face -- is defined as a loop. The geometric portion consists of equations defining the edges and faces. There are a number of different ways in which the geometry of an object's surface area can be modeled, but the most common is the Non-Uniform Rational Basis Spline (NURBS).

\subsubsection{Non-Uniform Rational Basis Spline (NURBS)}
NURBS curves are a method of interpolating a discrete set of points into a continous function. 








\begin{figure}
	\centering
		\includegraphics{cadTypes/brep.jpg}
	\caption[Components of a boundary representation.]{\centering An overview of the components involved in a boundary representation of a solid object \cite{Stroud2006}.}
\end{figure}


\subsubsection{Delaunay Triangulation}
Surface meshing is the science of inferring a continuous shape topology from a discrete, $n$-dimensional point cloud. There are many different approaches to converting from discrete points to surface meshes, but at their core, nearly all of them rely on the Delaunay triangulation method.
Delaunay triangulation finds its routes in Voronoi tessellation, a method of constructing non-overlapping geometrical tiles. Voronoi tessellation states the following:
For a given set of points in space, $\{P_{k}\}$ --- $k$ = 1, \ldots, $K$, the regions \{$V_{k}$\} are polygons assigned to each seed point $P_{k}$, such that $V_{k}$ represents the space closer to $P_{k}$ than any other point in the set.

\begin{equation}
	V_{k} = \{P_{i} \mid |p - P_{i}| < |p - P_{j}|, \forall j \neq i \}
\end{equation}

If every point pair sharing a Voronoi boundary are connected, the result is a triangulation object encasing the pointset. This object is referred to as a Delaunay triangulation \cite{RN65}.

\begin{figure}[!ht]
\centering
\includegraphics{delaunay.png}
\caption[2D delaunay triangulation]{\centering Visual represention of a 2-dimensional delaunay triangulation process}
\end{figure}

\subsubsection{Advancing Front / Marching Triangles}
The Advancing Front method is common in computer graphics software – especially in procedurally generated games – because of it’s speed and computational simplicity.
[DEFINITION, USAGE, AND 2D EXAMPLE]
\cite{RN66}.

\subsubsection{Scale Space Reconstruction Method}
At a grand scale, the Scale Space Reconstruction Method aims to optimize surface meshing in the face of discrete point cloud data. No matter how accurate or dense a point cloud may be, there is no way to verify the topology defined by the cloud is accurate to the true object topology. To simplify this ill-posed problem, and reduce mesh quality damage due to noisy points, the Scale Space algorithm casts the raw point cloud to a space of scale N by iteratively calculating the mean curvature of a neighborhood of points and casting each point pk to it’s nearest point on the curve. This results in a far more uniform point cloud, which can mesh via Delaunay triangulation at a high quality level. Once the meshing has occurred, the pointset is then recast to its original scale to maintain complex features. 

\textbf{Algorithm 1:} Mean Curvature Calculation

	\textbf{Input:} A point set $P$, a query point $p$, and a radius $r$.
	
	\textbf{Output:} A point $p'$, the result of one discrete step of the mean curvature calculation applied to $p$.
\begin{lstlisting}[escapeinside={(*}{*)}]
for ( (*$p$*) in (*$P$*) )
	get (*$neighbors$*) from (*$p$*)
	if num(neighbors) < 5
		remove (*$p$*)
	set (*$p_{bar} = \frac{\sum_{q \in neighbors} w(q)q}{\sum_{q \in neighbors}w(q)}$*)
	set (*$C = \sum_{q \in neighbors} w(q)(q-p_{bar})(q-p_{bar})^{2}$*)
	set (*$v_{0} = arg_{min} eigenvector(C)$*)
	set (*$p' = p - \langle p - p_{bar}, v_{0} \rangle v_{0}$*)
	(*$p' \cdot n = \frac{p-p'}{|p-p'|} \cdot sign(\langle p - p', p \cdot n \rangle ) $*	
\end{lstlisting}

\textbf{Algorithm 2:} Scale Space Iterator

	\textbf{Input:} A point set $P$, a number of iterations $N$, and a radius $r$
	
	\textbf{Output:} A modified point set $P_{N}$


\begin{lstlisting}[escapeinside={(*}{*)}]
for (*$p$*) in (*$P$*)
	set (*$p.origin = p$*)
set idx = 0
for ( i = 0, \ldots, N-1 )
	new_idx = mod(idx, 2) + 1
	for (*$p \in P_{idx}$*)
		(*$p' = MCC(p, P_{idx}, r)$*)
		store (*$p'$*) in (*$ P_{new_idx}$*)
		(*$p'.origin = p.origin $*)
	if ( idx > 0 )
		remove (*$P_{idx}$*)
	idx = new_idx
\end{lstlisting}

\textbf{Algorithm 3:} Back Projection to Final Mesh

	\textbf{Input:} A point set $P$, a number of iterations $N$, and a radius $r$
	
	\textbf{Output:} A modified point set $P_{N}$


\begin{lstlisting}[escapeinside={(*}{*)}]
\end{lstlisting}
\cite{RN67}

\begin{table}[h]
\centering
\caption[Effect of increasing scale in a scale space reconstruction]{Effect of increasing scale in a scale space reconstruction}
\begin{tabular}{ | c | c | c | }
\hline
& \includegraphics[width=3cm]{l_block_pt_cloud.jpg}
	Clean Point Cloud
& \includegraphics[width=3cm]{l_block_pt_cloud10pnoise.jpg}
	10\% Induced Noise
\\
\hline
Scale Space 3 & \includegraphics[width=3cm]{scalespace/clean/scalespace3.png} & \includegraphics[width=3cm]{scalespace/10pnoise/scale3.png}
\\
\hline
Scale Space 5 & \includegraphics[width=3cm]{scalespace/clean/scalespace5.png} & \includegraphics[width=3cm]{scalespace/10pnoise/scale5.png}
\\
\hline
Scale Space 10 & \includegraphics[width=3cm]{scalespace/clean/scalespace10.png} & \includegraphics[width=3cm]{scalespace/10pnoise/scale10.png}
\\
\hline
\end{tabular}
\end{table}


 

\subsubsection{Hole Patching, Fairing, and Refinement}

\subsection{Mesh Optimization}
Now that the objects in the cloud are properly segmented, they must be meshed in a way that is both accurate to the real-world definition of the object and suitable to be converted from a surface mesh to a volumetric mesh.

\subsubsection{Criteria for Mesh Quality and Failure Criteria for Volumetric Conversion}
Now that the objects in the cloud are properly segmented, they must be meshed in a way that is both accurate to the real-world definition of the object and suitable to be converted from a surface mesh to a volumetric mesh.

\subparagraph{Failure Criteria}
The first criterion for surface mesh compatibility with volumetric conversion is “water-tightness.” Meaning, there are no gaps, holes, or unbounded edges in the triangulation. These gaps can be quantified as any edge in a polyhedron that is referenced by no other polyhedron in the triangulation.
The other criteria are more abstract in nature and are more difficult to detect and handle independently. We define these criteria as mesh “Quality.” Quality is a quantification of the level of simplicity of a triangulation object by evaluating ratios of different elements in the triangulation.

\begin{figure}[!ht]
	\centering
		\includegraphics[width=2in]{triangulation_definitions.JPG}
		\caption[Definitions for triangulation quality measures]
		{\centering Definitions for triangulation quality measures. $R$ = circumsphere radius, $r$ = inscribed sphere radius, $l$ = edge length.}
		\label{fig:meshquality}
\end{figure}

\begin{table}
	\centering
		\caption{Quantification of mesh quality}
		\begin{tabular}{| c | c |}
			\hline
			Inner/outer radius edge ratios & $Q_{1} = \frac{l_{min}}{R}$, $Q_{2} = \frac{r}{l_{min}}$
			\\ \hline
			Aspect ratio & $Q_{3} = \frac{r}{R}$
			\\ \hline
			Edge ratio & $Q_{4} = \frac{l_{min}}{l_{max}}$
			\\ \hline
			Volume ratio & $Q_{5} = \frac{V}{l_{max}^{3}}$
			\\ \hline
		\end{tabular}
\end{table}

\begin{figure}[!ht]
	\centering
		\includegraphics[width=3in]{bad_tetrahedra.JPG}
		\caption[Examples of poor quality tetrahedra]
		{\centering Common examples of poor quality tetrahedra \cite{RN68}.}
		\label{fig:badtetrahedra}
\end{figure}

Low  quality  values  in  mesh  return  problematic  polyhedrons  for  volumetric  conversion,  typically  looking  like  those  shown  in  figure \ref{fig:badtetrahedra}.



\subsubsection{Voronoi Relaxation / Lloyd's Algorithm}

\begin{figure}[!ht]
	\centering
		\includegraphics[width=3in]{voronoirelaxation.PNG}
		\caption[Demonstration of voronoi relaxation over several iterations]
		{\centering From right to left: Voronoi relaxation over 1 iteration, 5 iterations, 10 iterations, and 15 iterations}
		\label{fig:voronoirelaxation}
\end{figure}

Voronoi  relaxation  operates  on  the  same  principle  as  k-means  clustering.  At  each  iteration,  the  centroid  of  each  Voronoi  region  is  calculated,  and  the  concurrent  vertex  is  moved  to  the  centroidal  location.  At  convergence,  the  resulting  mesh  is  uniform  in  tetrahedron/triangulation  size.  Voronoi  relaxation  can  modify  a  shape’s  topology  significant  due  to  its  heavy  smoothing  capabilities  \cite{RN61}.

\subsubsection{Optimal Delaunay Triangulation}

\subsubsection{Mesh Perturbation}
While  Voronoi  relaxation  and  ODT  are  large  scale  smoothing  and  refinement  techniques,  they  have  no  constraints  on  slivers  present  in  the  mesh.  Perturbation  and  exudation  are  are  necessary  to  oust  any  slivers  remaining  in  the  triangulation.  Slivers  are  defined  as  any  triangulation  with  an  angle  less  than  $\alpha$,  a  user-defined  parameter.  The  algorithm  iteratively  increases  the  angles  created  in  a  triangulation  by  applying  a  pseudo-random  perturbation  vector,  $p_{v}$,  to  vertices  coincident  with  triangulations  defined  as  slivers.  If  the  perturbation  results  in  a  success,  resulting  triangulation  is  kept.  Otherwise,  a  new  perturbation  vector  is  calculated  to  create  a  higher  quality  triangulation  \cite{RN63}. 

\subsubsection{Mesh Exudation}
Exudation  again  is  a  method  to  remove  any  slivers  remaining  in  the  surface.  Each  point  in  a  tetrahedron  classified  as  a  sliver  is  assigned  a  weight  based  on  its distance  relationship  to  it’s  neighbors.  The  point  weighted  most  heavily,  then,  is  the  tip  of  the  sliver,  and  is  modified  to  place  the  tetrahedron  within  the  acceptable  bounds  of  mesh  quality  \cite{RN64}. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Chapter: 		Hypothesis
\chapter{Objective, Hypothesis, and Technical Approach}
\label{chap:purposestatement}
Autonomously isolated objects from point cloud data can be analyzed from a structural health monitoring perspective. 

\section{Research Objective}
The objective of this thesis is to develop and validate a software pipeline capable of segmenting point cloud data of some expected density $\rho$ and within some accuracy tolerance $\sigma$ into clusters of meaningful shapes, and convert the resulting clusters into surface meshes valid for conversion to simply connected volumes. The research described in this thesis is inarguably very wide in application and scope, but is geared specifically towards Structural Health Monitoring and Non-Destructive Test and Analysis. The results of this Master's thesis are a proving ground for rapid scanning, meshing, and analysis of real-world objects.

\section{Hypothesis Statement}
The claims made in this thesis are the following:

\begin{enumerate}
	\item Point cloud scans oriented towards a specific object can be segmented to isolate said object with or without supervised clustering methods.
	\item Providing a point cloud meets some required criteria, there is a generalized algorithm to convert raw clouds to volumetrically adaptable surface meshes.
\end{enumerate}

The second hypothesis attempts to nail down an explicit definition of what it means for a mesh to be volumetrically compatible. Determining what conditions a point cloud must meet to be properly meshed, and what modifications can be made and to what extent are all problems that have yet to be fleshed out completely in the domain of real world to simulation conversion technology. This thesis aims to quantify criteria for raw point clouds to be meshed properly, criteria for smoothing limits before the resulting volume can no longer be realistically considered to have similar properties to it's real world counterpart, and criteria for a surface mesh to be successful converted to a volume. 

Knowing these limits allows for collection of point cloud data for the sake of information extraction to be much more educated. Defining criteria for the amount of a surface that must be visible in the cloud, accuracy limits, and smoothing limits are critical for creating an automated approach to finite element analysis on real objects with reliable results. The following chapters quantify the effect of parameter modification at each step of the algorithm.

\section{Technical Approach}
To fully evaluate the effect of each step in the algorithm and test the robustness of the proposed algorithm, we apply it to a series point clouds, but simulated and real, with varying levels of noise and occlusion. The goal is to accurately mimic the cloud resolution of the test device, and to proof the robustness of the algorithm at resolutions expected from other measurement devices. Simulated data has been crafted to model resolution, accuracy, and occlusion levels with the approaches that follow:

\subsection{LiDAR: Velodyne HDL-32E}
The Velodyne HDL-32E is a high end LiDAR device capable of collecting 700,000 points per second at a frame rate of 20 Hz (rotations per second). It is equipped with 32 lasers aligned from +\ang{10.67} to -\ang{30.67}

Like all LiDAR devices, the HDL-32E does not return cartesian co-ordinate cloud, but instead a spherical co-ordinate cloud, where each point is defined by $\phi$, it's angle about the $z$-axis, $\theta$, it's angle about the $y$-axis, and $r$, the absolute distance from the sensor. The return can be classified as ordered data, and easily mimicked in simulation by creating a series of solid objects, simulating the output of the LiDAR, and simulating the LiDAR motion. This allows for fast simulation of point cloud collection with varying levels of noise, occlusion, and sample rate.

\begin{figure}[!ht]
	\centering
		\includegraphics[width=2in]{cloudCollection/velodyneLiDAR.jpg}
		\caption[Velodyne LiDAR Operation]{Visualization of how to the Velodyne HDL-32E operates.}
\end{figure}








%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%Chapter			Results
\chapter{Results}
\label{chap:results}
\section{Simulated Data: Zero Noise}

\begin{figure}[!ht]
	
		\centering
		\includegraphics[width=3in]{simulated-lab-scan/0noise/sim-lab-0noise.jpg}
		\caption[Simulated laboratory point cloud data with zero noise induced]{\centering Simulated laboratory point cloud data with zero noise induced.}
	\label{zeronoise:raw}
\end{figure}

\begin{figure}[!ht]
	
	\centering
		\includegraphics[width=3in]{simulated-lab-scan/0noise/sim-lab-0noise-DS01-k15std1.jpg}
		\caption[Zero noise simulated data after being filtered with voxel size of 0.1 $cm^{3}$, 15 minimum point neighbors at a distance of 1 standard deviation from the mean point to point distance]{\centering Zero noise simulated data after being filtered with voxel size of $0.1 cm^{3}$, 15 minimum point neighbors at a distance of 1 standard deviation from the mean point to point distance.}
	\label{zeronoise:filtered}
\end{figure}



\subsection{Segmentation Method Comparison}

\begin{figure}[!ht]
	\label{zeronoise:segcompare}
	\centering
		\includegraphics[width=3in]{simulated-lab-scan/0noise/all_methods.jpg}
		\caption[Comparison of unsupervised segmentation techniques on a simulated dataset.]{\centering Resulting cloud cluster from a) k-means, 17 centroid b) Fuzzy c-means, 17 centroids c) Agglomerative clustering, 17 bins and d) Euclidean clustering with radius of 0.5 cm and a maximum cluster size of 50,000 points.}
	\label{zeronoise:optimal}
\end{figure}



\begin{figure}[!ht]
	
	\centering
		\includegraphics[width=3in]{simulated-lab-scan/0noise/euclidean-d05max50000min3000.jpg}
		\caption[Euclidean distance segmentation with ideal parameters.]{\centering  Euclidean clustering with radius of 0.5 cm, minimum cluster size of 3,000 points and a maximum cluster size of 50,000 points.}

\end{figure}


\subsection{Initial Meshing Methods}
In a zero noise situation, the advancing front method returns a nearly perfect mesh, with the exception of a poor quality polygon linking the rear segment of the mesh. This is caused by the ''marching triangles'' attempting to close the final corners of the mesh. This meshing method provides high quality meshes while maintaining accurate features in zero noise situations, but we will see when we introduce noise that Advancing Front quickly fails to produce a reliable initial mesh for successful optimization.

\begin{figure}[!ht]
	
	\centering
		\includegraphics[trim={5in 2in 3in 3in},clip,width=2in]{simulated-lab-scan/0noise/clean/advancingfront.png}
		\includegraphics[trim={0 3in 0 3in},clip,width=6in]{simulated-lab-scan/0noise/clean/advancingfront01.png}
		\includegraphics[width=2in]{simulated-lab-scan/0noise/clean/advancingfront02.png}
		\caption[Initial meshing using a raw advancing front approach]{\centering  Result of initial meshing using the advancing front method.}
		\label{zeronoise:advancingfront}
\end{figure}

In Figure~\ref{zeronoise:scalespace2} we see the effects of Scale Space reconstruction at scale $S = 2$. The predominantly visible effect is the change in topology of the beam. because of the localized fnunction estimation that occurs during the scale-space casting phase, the planar surface of the beam is reconstructed with a parabolic shape. The overall thickness is reduced.

\begin{figure}[!ht]
	\centering
		\includegraphics[trim={5in 2in 3in 3in},clip,width=2in]{simulated-lab-scan/0noise/clean/scale200.png}
		\includegraphics[width=6in]{simulated-lab-scan/0noise/clean/scalespace201.png}
		\includegraphics[width=2in]{simulated-lab-scan/0noise/clean/scalespace202.png}
		\caption[Initial meshing using a scale space reconstruction with $S = 2$]{\centering  Result of initial meshing using the scale space reconstruction method with $S = 2$.}
	\label{zeronoise:scalespace2}
\end{figure}

As the beam is cast into higher and higher scale-spaces, the thickness of the beam is continually reduced. At scale-space 4 and higher -- shown in Figures~\ref{zeronoise:scalespace4} and \ref{zeronoise:scalespace15}, our beam thickness (originally $\frac{1}{4}$in) is reduced to zero. This is due again to the piecewise function estimation that occurs in the specified scale space. Once all of the points are cast down far enough, they are no longer registered as being in two separate neighborhoods, and are instead identified as belonging to the same 2-dimensional curve.

\begin{figure}[!ht]
	\centering
		\includegraphics[trim={5in 2in 3in 3in},clip,width=2in]{simulated-lab-scan/0noise/clean/scalespace400.png}
		\includegraphics[width=6in]{simulated-lab-scan/0noise/clean/scalespace401.png}
		\includegraphics[trim={0.5in 1in 2in 2in}, clip, width=2in]{simulated-lab-scan/0noise/clean/scalespace402.png}
		\caption[Initial meshing using a scale space reconstruction with $S = 4$]{\centering  Result of initial meshing using the scale space reconstruction method with $S = 4$.}
		\label{zeronoise:scalespace4}
\end{figure}

\begin{figure}[!ht]
	\centering
		\includegraphics[trim={5in 2in 3in 3in},clip,width=2in]{simulated-lab-scan/0noise/clean/scalespace1500.png}
		\includegraphics[width=6in]{simulated-lab-scan/0noise/clean/scalespace1501.png}
		\includegraphics[trim={1in 1in 2in 2in}, clip, width=2in]{simulated-lab-scan/0noise/clean/scalespace1502.png}
		\caption[Initial meshing using a scale space reconstruction with $S = 15$]{\centering  Result of initial meshing using the scale space reconstruction method with $S = 15$.}
		\label{zeronoise:scalespace15}
\end{figure}



\subsection{Optimization}
\subsubsection{Effects Voronoi Relaxation}


\begin{figure}[!ht]
	\centering
		\includegraphics[trim={6in 2in 3in 2in},clip,width=2in]{simulated-lab-scan/0noise/optimized/advancedfrontLloyd30s200it00.png}
		\includegraphics[width=6in]{simulated-lab-scan/0noise/optimized/advancedfrontLloyd30s200it01.png}
		\includegraphics[trim={1in 1in 2in 2in}, clip, width=2in]{simulated-lab-scan/0noise/optimized/advancedfrontLloyd30s200it02.png}
		\includegraphics[trim={1in 1in 1in 2in}, clip, width=2in]{simulated-lab-scan/0noise/optimized/advancedfrontLloyd30s200it03.png}
		\caption[Advancing Front mesh after 200 iterations of voronoi relaxation]{\centering  Result of Advancing Front mesh after 200 iterations of voronoi relaxation.}
	\label{zeronoise:advancedlloyd}
\end{figure}


\begin{figure}[!ht]
	\centering
		\includegraphics[trim={6in 2in 3in 3in},clip,width=2in]{simulated-lab-scan/0noise/optimized/scalespace2Lloyd30s200it00.png}
		\includegraphics[trim={6in 2in 3in 3in},clip,width=2in]{simulated-lab-scan/0noise/optimized/scalespace2Lloyd30s200it01.png}
		\includegraphics[trim={6in 2in 3in 3in},clip,width=2in]{simulated-lab-scan/0noise/optimized/scalespace2Lloyd30s200it02.png}
		\includegraphics[trim={3in 2in 3.5in 3in},clip,width=2in]{simulated-lab-scan/0noise/optimized/scalespace2Lloyd30s200it03.png}
		\caption[Scale space reconstruction $S = 2$ after 200 iterations of voronoi relaxation]{\centering [Scale space reconstruction $S = 2$ after 200 iterations of voronoi relaxation.}
	\label{zeronoise:scalespace2lloyd}
\end{figure}


\begin{figure}[!ht]
	\centering
		\includegraphics[trim={6in 2in 2in 3in},clip,width=2in]{simulated-lab-scan/0noise/optimized/scalespace4Lloyd30s200it00.png}
		\includegraphics[trim={6in 2in 3in 3in},clip,width=2in]{simulated-lab-scan/0noise/optimized/scalespace4Lloyd30s200it01.png}
		\includegraphics[trim={4.5in 2in 3in 3in},clip,width=2in]{simulated-lab-scan/0noise/optimized/scalespace4Lloyd30s200it02.png}
		\includegraphics[trim={3in 4in 3.5in 4in},clip,width=2in]{simulated-lab-scan/0noise/optimized/scalespace4Lloyd30s200it03.png}
		\caption[Scale space reconstruction $S = 4$ after 200 iterations of voronoi relaxation]{\centering Scale space reconstruction $S = 4$ after 200 iterations of voronoi relaxation.}
	\label{zeronoise:scalespace4lloyd}
\end{figure}

\subsection{Effects of Delaunay Optimization}


\subsection{Effects of Mesh Perturbation}

\begin{figure}[!ht]
	\centering
		\includegraphics[trim={4in 2in 3in 2in},clip,width=2in]{simulated-lab-scan/0noise/optimized/advancedfrontperturbvslloyd00.png}
		\includegraphics[trim={1in 1in 2in 2in}, clip, width=2in]{simulated-lab-scan/0noise/optimized/advancedfrontperturbvslloyd01.png}
		\includegraphics[trim={1in 1in 1in 2in}, clip, width=2in]{simulated-lab-scan/0noise/optimized/advancedfrontperturbvslloyd02.png}
		\caption[Comparison of mesh before and after perturbation]{\centering  Comparison of mesh before and after perturbation.}
	\label{zeronoise:advancedlloydperturb}
\end{figure}


\begin{figure}[!ht]
	\centering
		\includegraphics[trim={1in 2in 1in 3in},clip,width=2in]{simulated-lab-scan/0noise/optimized/scalespace2lloydvsperturb00.png}
		\includegraphics[trim={2in 2in 2in 2in},clip,width=2in]{simulated-lab-scan/0noise/optimized/scalespace2lloydvsperturb01.png}
		\includegraphics[trim={1in 1in 3in 3in},clip,width=2in]{simulated-lab-scan/0noise/optimized/scalespace2lloydvsperturb02.png}
		\caption[Scale space reconstruction $S = 2$ after 200 iterations of voronoi relaxation and with perturbation]{\centering Scale space reconstruction $S = 2$ after 200 iterations of voronoi relaxation and with perturbation.}
	\label{zeronoise:scalespace2lloydperturb}
\end{figure}




\section{Simulated Data: Applied Gaussian noise $\pm 2 cm$}
\begin{figure}[!ht] 
	
	\centering
		\includegraphics[width=3in]{simulated-lab-scan/2cmnoise/rawcloud.jpg}
		\caption[Simulated point cloud with 2cm magnitude gaussian noise induced]{\centering  Simulated point cloud with 2cm magnitude gaussian noise induced.}
\label{2cmnoise:raw}
\end{figure}

\begin{figure}[!ht]
	
	\centering
		\includegraphics[width=3in]{simulated-lab-scan/2cmnoise/DS01k15std1.jpg}
		\caption[2 cm gaussian noise simulated data after being filtered with voxel size of 0.1 $cm^{3}$, 15 minimum point neighbors at a distance of 1 standard deviation from the mean point to point distance]{\centering 2 cm gaussian noise simulated data after being filtered with voxel size of $0.1 cm^{3}$, 15 minimum point neighbors at a distance of 1 standard deviation from the mean point to point distance.}
\label{2cmnoise:filtered}
\end{figure}

\subsection{Segmentation Method Comparison}

\begin{figure}[!ht]
	
	\centering
		\includegraphics[width=3in]{simulated-lab-scan/2cmnoise/comparison.jpg}
		\caption[Comparison of unsupervised segmentation techniques on simulated dataset with induced noise.]{\centering Resulting cloud cluster from a) k-means, 17 centroid b) Fuzzy c-means, 17 centroids c) Agglomerative clustering, 17 bins and d) Euclidean clustering with radius of 0.5 cm minimum cluster size of 3,000 and maximum cluster size of 50,000 points.}
\label{2cmnoise:segcompare}
\end{figure}


\subsection{Initial Mesh Methods}

\begin{figure}[!ht]
	
	\centering
		\includegraphics[trim={4.5in 1in 1in 3in},clip,width=3in]{simulated-lab-scan/2cmnoise/2cmmesh/advancingfront00.png}
		\includegraphics[width=6in]{simulated-lab-scan/2cmnoise/2cmmesh/advancingfront01.png}
		\includegraphics[trim={1in 1in 2in 2in}, clip, width=2in]{simulated-lab-scan/2cmnoise/2cmmesh/advancingfront02.png}
		\caption[Initial meshing using a raw advancing front approach]{\centering  Result of initial meshing using the advancing front method.}
		\label{2cmnoise:advancingfront}
\end{figure}


\begin{figure}[!ht] 
	
	\centering
		\includegraphics[trim={5in 2in 1.5in 3in},clip,width=3in]{simulated-lab-scan/2cmnoise/2cmmesh/scalespace200.png}
		\includegraphics[width=6in]{simulated-lab-scan/2cmnoise/2cmmesh/scalespace201.png}
		\includegraphics[trim={1in 1in 2in 2in}, clip, width=2in]{simulated-lab-scan/2cmnoise/2cmmesh/scalespace202.png}
		\caption[Initial meshing using a scale space reconstruction with $S = 2$]{\centering  Result of initial meshing using the scale space reconstruction method with $S = 2$.}
		\label{2cmnoise:scalepspace2}
\end{figure}


\begin{figure}[!ht]
	
	\centering
		\includegraphics[trim={4.5in 2in 1.5in 3in},clip,width=3in]{simulated-lab-scan/2cmnoise/2cmmesh/scalespace400.png}
		\includegraphics[width=6in]{simulated-lab-scan/2cmnoise/2cmmesh/scalespace401.png}
		\includegraphics[trim={1in 1in 2in 2in}, clip, width=2in]{simulated-lab-scan/2cmnoise/2cmmesh/scalespace402.png}
		\caption[Initial meshing using a scale space reconstruction with $S = 4$]{\centering  Result of initial meshing using the scale space reconstruction method with $S = 4$.}
		\label{2cmnoise:scalespace4}
\end{figure}


\begin{figure}[!ht]
	
	\centering
		\includegraphics[trim={4in 2in 1.5in 3in},clip,width=3in]{simulated-lab-scan/2cmnoise/2cmmesh/scalespace1500.png}
		\includegraphics[width=6in]{simulated-lab-scan/2cmnoise/2cmmesh/scalespace1501.png}
		\includegraphics[trim={1in 1in 2in 2in}, clip, width=2in]{simulated-lab-scan/2cmnoise/2cmmesh/scalespace1502.png}
		\caption[Initial meshing using a scale space reconstruction with $S = 15$]{\centering  Result of initial meshing using the scale space reconstruction method with $S = 15$.}
		\label{2cmnoise:scalespace15}
\end{figure}

\subsection{Optimization}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%Chapter: 		Conclusion
\chapter{Conclusion}
\label{chap:conclusion}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%Chapter: 		Future Work
\chapter{Future Work}
\label{chap:future}







%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\pagebreak
\addcontentsline{toc}{chapter}{Bibliography}
\begin{spacing}{1.0}
\bibliographystyle{IEEEtranN}
\bibliography{references}
\end{spacing}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\thispagestyle{empty}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
                                                                                                
\end{document}